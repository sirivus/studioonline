

BUGS:
+ docgen: <code> empty
+ docgen: name mangling for Object in JavaDoc


CODE:
+ allow to differentiate between read/write access when opening
+ separate source code generator from I/O lib
+ develop and implement conversion concept


BUILD ENVIRONMENT:
+ common approach to retrieving blender version in scripts and maven build



DOCUMENTATION:
- Blender version and blender file version.
- userprefs.blend no longer exists
- Add link to wiki in documentation: https://wiki.blender.org/wiki/Source/File_Structure





CONVERSION CONCEPT (WIP):
=========================
The big question here is: Is it really helpful?

Compatibility is usually quite limited. 
  - There are a number of changes, such as changed names of member variables, 
    which cannot be covered by conversion and it's unknown, what the actual 
    effects of those changes are in terms of compatibility.



- decide, whether conversion is necessary based on supported version range and DNA in the file
- differentiate between read/write access to decide, whether 
  (write) we just convert the whole file or 
  (readonly) we convert on demand into a temporary block 
  or just do the first one always and keep readonly access for later.

Approaches to conversion
- Approach A
  - Have an empty target memory region attached to the target facade
    - memory region has to be allocated with a block
    - size has to be determined based on the DNA associated with the 
      DNA lib and the number of items to be stored.
    - this cannot be circumvented in readonly case, because we have 
      to make changes to data dynamically (e.g. in case of pointers).
  - Dynamically interpret source memory region based on the DNA given 
    in the file (source).
  - Use facade to write to the target memory region based on the DNA 
    (sdna.blend) coming with the DNA lib.
  - Keep track of mappings between old and new addresses of converted structs.
    Since sizes of structs can be different, data will get new addresses 
    after conversion and putting the new blocks at the same address as the
    old one, would result in overlapping blocks. This would cause issues, even
    though it is allowed in some places but not generally.
    Thus, we have to have a method to map old addresses to new addresses.
  - Dynamically resolve pointers to new memory regions.
    There is no other choice, because we can't know which address 
    data will have after conversion and in many cases we don't even 
    know, that some of the converted data will be interpreted as a 
    pointer.
  - When creating pointers they will always point into old data at first.
    When originally designing CPointer we already decided to always
    associate a BlenderFile instance with it. When receiving a pointer from
    a struct which has been converted, the address of the pointer will still
    point into the old file. Thus, the pointer actually has to be associated 
    with the old file, so we know, that we have to convert the target and adjust
    the pointer with the new address, before using it. When it comes to arrays, 
    this becomes more difficult, because we usually don't know the length of the
    array and can't just convert the whole thing. Thus, when accessing the 
    next element in the array, after we have converted the first one and adjustment of
    the address, we have to know where this array is located in the old file again. So,
    we would need the old address again.
- Approach B
  - Rework generator and I/O module to have a native Java representation of all structs (not just facades)
  - Always interpret data coming from the file based on the DNA given in the file.
  - Always write data based on the DNA comprised with the DNA lib (sdna.blend)
  - I believe, this approach has the same issues as the one above.



   